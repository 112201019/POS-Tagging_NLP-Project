{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu@NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"Telugu@NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['hello', 'lights', 'dim', 'cēyaṇḍi']\n",
      "POS Tags: ['NOUN', 'NOUN', 'VERB', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "import conllu\n",
    "\n",
    "# Function to load and parse a CoNLL-U file\n",
    "def load_conllu_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = conllu.parse(f.read())\n",
    "    return data\n",
    "\n",
    "# Load the train, dev, and test files\n",
    "train_data = load_conllu_data('UD_Telugu_English-TECT-master/qte_tect-ud-train.conllu')\n",
    "test_data = load_conllu_data('UD_Telugu_English-TECT-master/qte_tect-ud-train.conllu')\n",
    "\n",
    "# Example: Print words and POS tags for the first sentence in the train data\n",
    "for sentence in train_data[:1]:  # Iterate over the first sentence\n",
    "    words = [token['form'] for token in sentence]\n",
    "    pos_tags = [token['upostag'] for token in sentence]\n",
    "    print(\"Words:\", words)\n",
    "    print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to extract sentences and POS tags\n",
    "def extract_sentences_and_tags(data):\n",
    "    sentences = []\n",
    "    pos_tags = []\n",
    "    for sentence in data:\n",
    "        words = [token['form'].lower() for token in sentence]  # Convert to lowercase\n",
    "        tags = [token['upostag'] for token in sentence]\n",
    "        sentences.append(words)\n",
    "        pos_tags.append(tags)\n",
    "    return sentences, pos_tags\n",
    "\n",
    "# Extract words and POS tags from each dataset\n",
    "train_sentences, train_pos_tags = extract_sentences_and_tags(train_data)\n",
    "test_sentences, test_pos_tags = extract_sentences_and_tags(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Encoded Sentence: [6, 7, 5]\n",
      "Sample Encoded POS Tags: [1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build word vocabulary with an <UNK> token\n",
    "word_vocab = defaultdict(lambda: len(word_vocab))  # Assigns unique IDs\n",
    "word_vocab[\"<PAD>\"] = 0  # Padding token\n",
    "word_vocab[\"<UNK>\"] = 1  # Unknown words\n",
    "\n",
    "# Build POS vocabulary\n",
    "pos_vocab = defaultdict(lambda: len(pos_vocab))  # Assigns unique IDs\n",
    "pos_vocab[\"<PAD>\"] = 0  # Padding token\n",
    "\n",
    "# Populate vocabularies using training data only\n",
    "for sentence, tags in zip(train_sentences, train_pos_tags):\n",
    "    for word in sentence:\n",
    "        _ = word_vocab[word]  # Assign ID if not already present\n",
    "    for tag in tags:\n",
    "        _ = pos_vocab[tag]  # Assign ID if not already present\n",
    "\n",
    "# Convert words and POS tags to IDs\n",
    "def convert_to_ids(sentences, pos_tags, word_vocab, pos_vocab):\n",
    "    encoded_sentences = [[word_vocab.get(word, word_vocab[\"<UNK>\"]) for word in sent] for sent in sentences]\n",
    "    encoded_pos_tags = [[pos_vocab[tag] for tag in tags] for tags in pos_tags]\n",
    "    return encoded_sentences, encoded_pos_tags\n",
    "\n",
    "# Convert datasets to numerical form\n",
    "train_sentences_ids, train_pos_tags_ids = convert_to_ids(train_sentences, train_pos_tags, word_vocab, pos_vocab)\n",
    "# dev_sentences_ids, dev_pos_tags_ids = convert_to_ids(dev_sentences, dev_pos_tags, word_vocab, pos_vocab)\n",
    "test_sentences_ids, test_pos_tags_ids = convert_to_ids(test_sentences, test_pos_tags, word_vocab, pos_vocab)\n",
    "\n",
    "# Print sample output\n",
    "print(\"Sample Encoded Sentence:\", train_sentences_ids[1])\n",
    "print(\"Sample Encoded POS Tags:\", train_pos_tags_ids[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Sentence: ['hello', 'lights', 'dim', 'cēyaṇḍi']\n",
      "Decoded POS Tags: ['NOUN', 'NOUN', 'VERB', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reverse mapping from ID to word/tag\n",
    "id_to_word = {idx: word for word, idx in word_vocab.items()}\n",
    "id_to_pos = {idx: tag for tag, idx in pos_vocab.items()}\n",
    "\n",
    "# Decode the first sentence\n",
    "decoded_words = [id_to_word[idx] for idx in train_sentences_ids[0]]\n",
    "decoded_tags = [id_to_pos[idx] for idx in train_pos_tags_ids[0]]\n",
    "\n",
    "print(\"Decoded Sentence:\", decoded_words)\n",
    "print(\"Decoded POS Tags:\", decoded_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Sentence: ['hello', 'lights', 'dim', 'cēyaṇḍi']\n",
      "Predicted POS Tags: ['NOUN', 'NOUN', 'VERB', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Step 1: Create a dictionary mapping words to their most frequent POS tag\n",
    "word_to_pos = defaultdict(lambda: \"NOUN\")  # Default to NOUN for unknown words\n",
    "pos_counts = defaultdict(Counter)  # Stores POS counts for each word\n",
    "\n",
    "# Populate the dictionary using training data\n",
    "for sentence, tags in zip(train_sentences, train_pos_tags):\n",
    "    for word, tag in zip(sentence, tags):\n",
    "        pos_counts[word][tag] += 1\n",
    "\n",
    "# Assign the most frequent POS tag for each word\n",
    "for word, counter in pos_counts.items():\n",
    "    word_to_pos[word] = counter.most_common(1)[0][0]\n",
    "\n",
    "# Function to predict POS tags for a given sentence\n",
    "def rule_based_tagger(sentence):\n",
    "    return [word_to_pos[word] if word in word_to_pos else \"NOUN\" for word in sentence]  # Default to NOUN\n",
    "\n",
    "# Example: Predict on a sentence from the test set\n",
    "sample_sentence = test_sentences[0]  # Take first test sentence\n",
    "predicted_tags = rule_based_tagger(sample_sentence)\n",
    "print(\"\\nSample Sentence:\", sample_sentence)\n",
    "print(\"Predicted POS Tags:\", predicted_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-Based POS Tagger Accuracy: 97.93%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rule_based_tagger(sentences, true_pos_tags):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for sentence, true_tags in zip(sentences, true_pos_tags):\n",
    "        predicted_tags = rule_based_tagger(sentence)\n",
    "\n",
    "        # Compare predictions with ground truth\n",
    "        for pred, true in zip(predicted_tags, true_tags):\n",
    "            if pred == true:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"Rule-Based POS Tagger Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Evaluate on dev set\n",
    "evaluate_rule_based_tagger(test_sentences, test_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Errors: 6\n",
      "\n",
      "Top 10 Most Misclassified Words:\n",
      "Word: enta | Predicted: ADV | True: ADJ | No. of times: 1\n",
      "Word: small | Predicted: ADJ | True: NOUN | No. of times: 1\n",
      "Word: kamala | Predicted: PROPN | True: NOUN | No. of times: 1\n",
      "Word: rāmu | Predicted: PROPN | True: NOUN | No. of times: 1\n",
      "Word: ikkaḍiki | Predicted: ADV | True: NOUN | No. of times: 1\n",
      "Word: telugu | Predicted: PROPN | True: NOUN | No. of times: 1\n",
      "\n",
      "Most Commonly Misclassified POS Tags:\n",
      "POS: NOUN | Errors: 5\n",
      "POS: ADJ | Errors: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_errors(sentences, true_pos_tags):\n",
    "    total_errors = 0\n",
    "    error_details = []\n",
    "    pos_error_counts = Counter()\n",
    "    \n",
    "    for sentence, true_tags in zip(sentences, true_pos_tags):\n",
    "        predicted_tags = rule_based_tagger(sentence)\n",
    "        \n",
    "        for word, pred, true in zip(sentence, predicted_tags, true_tags):\n",
    "            if pred != true:  # Error detected\n",
    "                total_errors += 1\n",
    "                pos_error_counts[true] += 1\n",
    "                error_details.append((word, pred, true))\n",
    "    \n",
    "    # Display overall error count\n",
    "    print(f\"Total Errors: {total_errors}\")\n",
    "    \n",
    "    # Display most common misclassified words\n",
    "    print(\"\\nTop 10 Most Misclassified Words:\")\n",
    "    for word, pred in Counter(error_details).most_common(10):\n",
    "        print(f\"Word: {word[0]} | Predicted: {word[1]} | True: {word[2]} | No. of times: {pred}\")\n",
    "\n",
    "    # Display which POS tags are misclassified the most\n",
    "    print(\"\\nMost Commonly Misclassified POS Tags:\")\n",
    "    for pos, count in pos_error_counts.most_common(10):\n",
    "        print(f\"POS: {pos} | Errors: {count}\")\n",
    "\n",
    "# Run analysis on dev set\n",
    "analyze_errors(test_sentences, test_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
